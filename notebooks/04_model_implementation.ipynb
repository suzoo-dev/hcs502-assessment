{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3af02fcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Matplotlib is building the font cache; this may take a moment.\n"
     ]
    }
   ],
   "source": [
    "# Setup imports\n",
    "from typing import Union\n",
    "import config\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pathlib\n",
    "import joblib\n",
    "import json\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from IPython.display import display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca10d6a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to load data\n",
    "def load_data_splits(dataset_prefix: str, base_path: pathlib.Path) -> tuple[pd.DataFrame, pd.DataFrame, pd.Series, pd.Series]:\n",
    "  \"\"\"\n",
    "  Loads the train/test data splits for a given dataset prefix.\n",
    "\n",
    "  Parameters:\n",
    "      dataset_prefix (str): The prefix for the dataset files (e.g., 'orig', 'rem', 'cap').\n",
    "      base_path (pathlib.Path): The path to the processed data directory from config.\n",
    "\n",
    "  Returns:\n",
    "      tuple: A tuple containing (X_train, X_test, y_train, y_test) as DataFrames/Series.\n",
    "  \"\"\"\n",
    "  X_train = pd.read_csv(base_path / f'X_{dataset_prefix}_train.csv')\n",
    "  X_test = pd.read_csv(base_path / f'X_{dataset_prefix}_test.csv')\n",
    "  y_train = pd.read_csv(base_path / f'y_{dataset_prefix}_train.csv').squeeze()\n",
    "  y_test = pd.read_csv(base_path / f'y_{dataset_prefix}_test.csv').squeeze()\n",
    "  return X_train, X_test, y_train, y_test\n",
    "\n",
    "# Load the data splits, origina, removed, and capped\n",
    "X_original_train, X_original_test, y_original_train, y_original_test = load_data_splits('original', config.PROCESSED_DATA_DIR)\n",
    "X_removed_train, X_removed_test, y_removed_train, y_removed_test = load_data_splits('removed', config.PROCESSED_DATA_DIR)\n",
    "X_capped_train, X_capped_test, y_capped_train, y_capped_test = load_data_splits('capped', config.PROCESSED_DATA_DIR)\n",
    "\n",
    "# Load the list of numerical features\n",
    "with open(config.PROCESSED_DATA_DIR / 'numerical_features_to_scale.json', 'r') as f:\n",
    "  numerical_features_to_scale = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "178f8c39",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scale_features(X_train: pd.DataFrame, X_test: pd.DataFrame, numerical_features: list) -> tuple[pd.DataFrame, pd.DataFrame, StandardScaler]:\n",
    "    \"\"\"\n",
    "    Applies StandardScaler to the numerical features of the dataset.\n",
    "    Scaler is fitted only on training data to prevent data leakage.\n",
    "\n",
    "    Parameters:\n",
    "        X_train (pd.DataFrame): The training feature set\n",
    "        X_test (pd.DataFrame): The testing feature set\n",
    "        numerical_features (list): Column names to be scaled\n",
    "\n",
    "    Returns:\n",
    "        tuple: (X_train_scaled, X_test_scaled, scaler_object)\n",
    "    \"\"\"\n",
    "    scaler = StandardScaler()\n",
    "    \n",
    "    # Create copies to avoid modifying the original dataframes\n",
    "    X_train_scaled = X_train.copy()\n",
    "    X_test_scaled = X_test.copy()\n",
    "    \n",
    "    # Fit on training data and transform both sets\n",
    "    X_train_scaled[numerical_features] = scaler.fit_transform(X_train[numerical_features])\n",
    "    X_test_scaled[numerical_features] = scaler.transform(X_test[numerical_features])\n",
    "    \n",
    "    return X_train_scaled, X_test_scaled, scaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70aecaed",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_evaluate_model(\n",
    "    model: Union[LinearRegression, KNeighborsRegressor, keras.Model],\n",
    "    X_train: pd.DataFrame,\n",
    "    y_train: pd.Series,\n",
    "    X_test: pd.DataFrame,\n",
    "    y_test: pd.Series,\n",
    "    dataset_name: str,\n",
    "    model_name: str\n",
    ") -> tuple[dict, Union[LinearRegression, KNeighborsRegressor, keras.Model]]:\n",
    "    \"\"\"\n",
    "    Trains a given model and evaluates its performance.\n",
    "\n",
    "    Parameters:\n",
    "        model: The machine learning model instance to train\n",
    "        X_train, y_train: The training data and labels\n",
    "        X_test, y_test: The testing data and labels\n",
    "        dataset_name: Name of the dataset treatment (e.g., 'Original')\n",
    "        model_name: Name of the model (e.g., 'Linear Regression')\n",
    "\n",
    "    Returns:\n",
    "        tuple: (results_dict, trained_model_object)\n",
    "    \"\"\"\n",
    "    # Train the model\n",
    "    if model_name == 'Neural Network':\n",
    "        model.fit(X_train, y_train, epochs=50, batch_size=32, verbose=0)\n",
    "        y_pred = model.predict(X_test).flatten()\n",
    "    else:\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "\n",
    "    # Calculate performance metrics\n",
    "    rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "    mae = mean_absolute_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "    # Store results in a dictionary\n",
    "    results = {\n",
    "        'Dataset': dataset_name,\n",
    "        'Model': model_name,\n",
    "        'RMSE': round(rmse, 2),\n",
    "        'MAE': round(mae, 2),\n",
    "        'R-squared': round(r2, 4)\n",
    "    }\n",
    "    \n",
    "    return results, model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a4c3368",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/homebrew/Caskroom/miniforge/base/envs/tf_env/lib/python3.10/site-packages/keras/src/layers/core/dense.py:93: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(activity_regularizer=activity_regularizer, **kwargs)\n",
      "2025-06-15 23:16:48.362647: I metal_plugin/src/device/metal_device.cc:1154] Metal device set to: Apple M4 Max\n",
      "2025-06-15 23:16:48.362762: I metal_plugin/src/device/metal_device.cc:296] systemMemory: 64.00 GB\n",
      "2025-06-15 23:16:48.362770: I metal_plugin/src/device/metal_device.cc:313] maxCacheSize: 24.00 GB\n",
      "2025-06-15 23:16:48.362959: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2025-06-15 23:16:48.362974: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n",
      "2025-06-15 23:16:49.083420: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:117] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m185/185\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 3ms/step\n",
      "Neural Network model performance:\n",
      "Dataset: Original\n",
      "Model: Neural Network\n",
      "RMSE: 530.68\n",
      "MAE: 139.62\n",
      "R-squared: 0.4831\n",
      "\n",
      "Comparison of all models:\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Dataset</th>\n",
       "      <th>Model</th>\n",
       "      <th>RMSE</th>\n",
       "      <th>MAE</th>\n",
       "      <th>R-squared</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Original</td>\n",
       "      <td>Linear Regression</td>\n",
       "      <td>542.75</td>\n",
       "      <td>149.03</td>\n",
       "      <td>0.4594</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Original</td>\n",
       "      <td>k-NN Regression</td>\n",
       "      <td>522.24</td>\n",
       "      <td>46.66</td>\n",
       "      <td>0.4995</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Original</td>\n",
       "      <td>Neural Network</td>\n",
       "      <td>530.68</td>\n",
       "      <td>139.62</td>\n",
       "      <td>0.4831</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Dataset              Model    RMSE     MAE  R-squared\n",
       "0  Original  Linear Regression  542.75  149.03     0.4594\n",
       "1  Original    k-NN Regression  522.24   46.66     0.4995\n",
       "2  Original     Neural Network  530.68  139.62     0.4831"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Define Models and Execute Training Pipeline\n",
    "def create_nn_model(input_shape: int) -> keras.Model:\n",
    "    \"\"\"\n",
    "    Creates and compiles a simple Keras Sequential model for regression.\n",
    "\n",
    "    This function defines a multi-layer perceptron (MLP) with two hidden\n",
    "    layers using the ReLU activation function, dropout for regularization,\n",
    "    and a final linear output layer suitable for predicting a continuous value.\n",
    "    The model is compiled with the Adam optimizer and Mean Squared Error loss.\n",
    "\n",
    "    Parameters:\n",
    "        input_shape (int): The number of features in the input data. This is\n",
    "                           used to correctly shape the initial Input layer.\n",
    "\n",
    "    Returns:\n",
    "        keras.Model: A compiled, untrained Keras model instance.\n",
    "    \"\"\"\n",
    "    model = keras.Sequential([\n",
    "        layers.Input(shape=(input_shape,)),\n",
    "        layers.Dense(128, activation='relu'),\n",
    "        layers.Dropout(0.2),\n",
    "        layers.Dense(64, activation='relu'),\n",
    "        layers.Dense(1) # Output layer for regression\n",
    "    ])\n",
    "    model.compile(optimizer='adam', loss='mean_squared_error')\n",
    "    return model\n",
    "\n",
    "# Define the models to be trained\n",
    "models_to_train = {\n",
    "    'Linear Regression': LinearRegression(),\n",
    "    'k-NN Regression': KNeighborsRegressor(n_neighbors=7),\n",
    "    'Neural Network': None  # Placeholder, will be created in the loop\n",
    "}\n",
    "\n",
    "# Prepare for the experiment loop \n",
    "datasets = {\n",
    "    'Original': (X_original_train, X_original_test, y_original_train, y_original_test),\n",
    "    'Removed Outliers': (X_removed_train, X_removed_test, y_removed_train, y_removed_test),\n",
    "    'Capped Outliers': (X_capped_train, X_capped_test, y_capped_train, y_capped_test)\n",
    "}\n",
    "\n",
    "all_results = []\n",
    "trained_artifacts = {} # To store trained models and scalers\n",
    "\n",
    "print(\"Starting model training pipeline for all 9 experiments...\")\n",
    "\n",
    "# --- Main Experiment Loop ---\n",
    "for d_name, (X_train, X_test, y_train, y_test) in datasets.items():\n",
    "    print(f\"\\n--- Processing Dataset: {d_name} ---\")\n",
    "    \n",
    "    # Step 1: Scale the features for this dataset\n",
    "    X_train_s, X_test_s, scaler = scale_features(X_train, X_test, numerical_features_to_scale)\n",
    "    trained_artifacts[f'{d_name}_scaler'] = scaler\n",
    "    \n",
    "    for m_name, model_instance in models_to_train.items():\n",
    "        print(f\"  -> Training Model: {m_name}...\")\n",
    "        \n",
    "        # Step 2: Train and evaluate the model\n",
    "        # A new Neural Network instance must be created for each run\n",
    "        if m_name == 'Neural Network':\n",
    "            current_model = create_nn_model(X_train_s.shape[1])\n",
    "        else:\n",
    "            current_model = model_instance\n",
    "        \n",
    "        results, trained_model = train_evaluate_model(\n",
    "            current_model, X_train_s, y_train, X_test_s, y_test, d_name, m_name\n",
    "        )\n",
    "        \n",
    "        # Step 3: Store results and artifacts\n",
    "        all_results.append(results)\n",
    "        trained_artifacts[f'{d_name}_{m_name}'] = trained_model\n",
    "\n",
    "print(\"\\nAll model training experiments completed.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f46de822",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compile, Display, and Save Results\n",
    "# Compile results into a DataFrame\n",
    "results_df = pd.DataFrame(all_results)\n",
    "\n",
    "print(\"\\nFinal Model Performance Comparison\")\n",
    "display(results_df.sort_values(by=['Dataset', 'RMSE']))\n",
    "\n",
    "# Create a pivot table for easier comparison\n",
    "pivot_results = results_df.pivot(index='Model', columns='Dataset', values=['RMSE', 'MAE', 'R-squared'])\n",
    "print(\"\\nPivot Table of Results\")\n",
    "display(pivot_results)\n",
    "\n",
    "# Save the results tables to the results folder\n",
    "results_df.to_csv(config.RESULTS_DIR / 'model_performance_summary.csv', index=False)\n",
    "pivot_results.to_csv(config.RESULTS_DIR / 'model_performance_pivot.csv')\n",
    "\n",
    "# Save all trained models and scalers\n",
    "print(\"\\nSaving all trained models and scalers...\")\n",
    "for name, artifact in trained_artifacts.items():\n",
    "    if 'scaler' in name:\n",
    "        joblib.dump(artifact, config.MODELS_DIR / f'{name}.joblib')\n",
    "    elif 'Neural Network' in name:\n",
    "        artifact.save(config.MODELS_DIR / f'{name}.keras')\n",
    "    else:\n",
    "        joblib.dump(artifact, config.MODELS_DIR / f'{name}.joblib')\n",
    "\n",
    "print(\"\\nAll artifacts saved successfully to the 'models/' directory.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a84dd00",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af16b89a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38305b8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3d7fe50",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (tf_env)",
   "language": "python",
   "name": "tf_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
